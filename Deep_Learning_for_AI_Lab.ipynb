{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMosoDPAF/NLvL6fTpcBWLh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Study of Deep Learning Packages: Tensorflow, Keras, Theano and PyTorch. Document the distinctfeatures and functionality of the packages."
      ],
      "metadata": {
        "id": "c_xR9KysY7E6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Use a suitable dataset forthe implementation of the following assignments."
      ],
      "metadata": {
        "id": "HBo42I3qZFV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing Feed-forward neural networks with Keras and TensorFlow\n"
      ],
      "metadata": {
        "id": "NhyIXJ-BZHic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theory\n",
        "\n",
        "Feed-forward neural networks are a type of artificial neural network where the information flows in one direction only, from the input layer to the output layer. Each layer of the network consists of a number of neurons, which are connected to the neurons in the next layer.\n",
        "\n",
        "The neurons in each layer perform a non-linear transformation of the input they receive from the previous layer. The output of each neuron is then passed to the next layer, until the output layer is reached.\n",
        "\n",
        "The output layer of the network produces the final output, which is a prediction of the class of the input data.\n",
        "\n",
        "Feed-forward neural networks are trained using a process called supervised learning. In supervised learning, we have a set of training data, which consists of input-output pairs. The goal is to train the network to produce the correct output for"
      ],
      "metadata": {
        "id": "0RGyfoQWuXg1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. Import the necessary packages\n",
        "\n"
      ],
      "metadata": {
        "id": "bt9iIQqZtVZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "z85TxQGLZsrb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "b. Load the training and testing data (MNIST/CIFAR10)\n",
        "\n"
      ],
      "metadata": {
        "id": "T-V6UFmktYCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Load the CIFAR10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
      ],
      "metadata": {
        "id": "yZcxnc1uZskA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c. Define the network architecture using Keras\n"
      ],
      "metadata": {
        "id": "I_8gbhSOtaXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "  Dense(128, activation='relu', input_shape=(784,)),\n",
        "  Dense(64, activation='relu'),\n",
        "  Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "j4OhWfpStf_D"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "d. Train the model using SGD\n",
        "\n"
      ],
      "metadata": {
        "id": "YcU9H3_gtdQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=100)"
      ],
      "metadata": {
        "id": "ohn4J38Itc5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "e. Evaluate the network\n",
        "\n"
      ],
      "metadata": {
        "id": "IL0QENMqtgr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "\n",
        "print('Test loss:', test_loss)\n",
        "print('Test accuracy:', test_accuracy)"
      ],
      "metadata": {
        "id": "TaZULJ0lZsg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "f. Plot the training loss and accuracy"
      ],
      "metadata": {
        "id": "U_7ad3ebti0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the training loss\n",
        "plt.plot(model.history.history['loss'])\n",
        "plt.title('Training loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "# Plot the training accuracy\n",
        "plt.plot(model.history.history['accuracy'])\n",
        "plt.title('Training accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VQsn6Nb4tkdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KPafbk3JtkO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the Image classification model by dividing the model into the following fourstages:\n"
      ],
      "metadata": {
        "id": "ipv0vnw5ZO2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theory\n",
        "\n",
        "Image classification models are typically trained using a supervised learning approach. In supervised learning, we have a set of training data, which consists of input-output pairs. The goal is to train the model to produce the correct output for a given input.\n",
        "\n",
        "In the case of image classification, the input is an image and the output is the class of the image. For example, the input image might be a picture of a cat and the output might be the class \"cat\".\n",
        "\n",
        "The model is trained by feeding it the training images and adjusting its parameters so that it produces the correct output for each image. The training process is typically iterative, meaning that we train the model for a number of epochs, and then evaluate the model’s performance on the validation data. If the model is not performing well on the validation data, we can adjust the model’s parameters and train the model for more epochs.\n",
        "\n",
        "Once the model has been trained, we can estimate its performance on the test data. This involves feeding the test images to the model and measuring the model’s accuracy. The accuracy of the model is the percentage of test images that the model correctly classifies."
      ],
      "metadata": {
        "id": "Oa7n7P3uu_y6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. Loading and preprocessing the image data\n",
        "\n"
      ],
      "metadata": {
        "id": "W2dZqgW7vEHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Preprocess the image data\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0"
      ],
      "metadata": {
        "id": "ahhIuVO7Zr0q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Defining the model’s architecture\n"
      ],
      "metadata": {
        "id": "7kgBFovAvGIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "model = Sequential([\n",
        "  Dense(128, activation='relu', input_shape=(784,)),\n",
        "  Dense(64, activation='relu'),\n",
        "  Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "CC02OLgDZrx1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "c. Training the model\n"
      ],
      "metadata": {
        "id": "ZkhFt-b2vIJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10)"
      ],
      "metadata": {
        "id": "_XEYMJbnZrvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "d. Estimating the model’s performance"
      ],
      "metadata": {
        "id": "jikRLjxpvKWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "\n",
        "print('Test loss:', test_loss)\n",
        "print('Test accuracy:', test_accuracy)"
      ],
      "metadata": {
        "id": "kbqCffCAvKxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6LxGPWADvNJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use Autoencoder to implement anomaly detection. Build the model by using the following:\n"
      ],
      "metadata": {
        "id": "hD7cHJKgZXB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theory**\n",
        "\n",
        "Autoencoders are a type of neural network that is trained to reconstruct its input. The encoder part of the autoencoder compresses the input data into a latent representation, and the decoder part of the autoencoder reconstructs the input data from the latent representation.\n",
        "\n",
        "Autoencoders can be used for anomaly detection by comparing the reconstructed output to the original input. If the reconstruction error is high, then the input data is likely to be anomalous."
      ],
      "metadata": {
        "id": "21mhN8utwOC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. Import required libraries\n",
        "\n"
      ],
      "metadata": {
        "id": "A_H_seeFvxLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "MqYL7tMQZrIT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Upload/access the dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "UPHUU4m7vzJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the MNIST dataset\n",
        "df = pd.read_csv('mnist.csv')\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_data = df.sample(frac=0.8, random_state=42)\n",
        "test_data = df.drop(train_data.index)"
      ],
      "metadata": {
        "id": "dHU1OGa4ZrFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c. The encoder converts it into a latent representation\n",
        "\n"
      ],
      "metadata": {
        "id": "OTt8qyiNv2UC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Sequential([\n",
        "  Dense(128, activation='relu'),\n",
        "  Dense(64, activation='relu'),\n",
        "  Dense(32, activation='relu')\n",
        "])"
      ],
      "metadata": {
        "id": "u6jXs2Svv1zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "d. Decoder networks convert it back to the original input\n",
        "\n"
      ],
      "metadata": {
        "id": "JWp3nZ8lv4nZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Sequential([\n",
        "  Dense(64, activation='relu'),\n",
        "  Dense(128, activation='relu'),\n",
        "  Dense(784, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "3tYB00J5v1kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "e. Compile the models with Optimizer, Loss, and Evaluation Metrics"
      ],
      "metadata": {
        "id": "GcTk-EQWv6m7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the encoder model\n",
        "encoder.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Compile the decoder model\n",
        "decoder.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "9o2W8PWZZwZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example"
      ],
      "metadata": {
        "id": "Fsg9JLMYwXFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the autoencoder model\n",
        "autoencoder = Sequential([\n",
        "  encoder,\n",
        "  decoder\n",
        "])\n",
        "\n",
        "autoencoder.fit(train_data, train_data, epochs=10)\n",
        "\n",
        "# Reconstruct the test data\n",
        "reconstructed_test_data = autoencoder.predict(test_data)\n",
        "\n",
        "# Calculate the reconstruction error\n",
        "reconstruction_error = np.mean(np.square(test_data - reconstructed_test_data))\n",
        "\n",
        "# Identify anomalies\n",
        "anomalies = test_data[test_data > reconstruction_error]\n"
      ],
      "metadata": {
        "id": "6f4adKRIv73o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MC-zt03EwbKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement the Continuous Bag of Words (CBOW) Model. Stages can be:\n"
      ],
      "metadata": {
        "id": "3cf_AA_IZcBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theory**\n",
        "\n",
        "The Continuous Bag of Words (CBOW) model is a neural network model that is used to generate word embeddings. Word embeddings are vectors of numbers that represent the meaning of words.\n",
        "\n",
        "The CBOW model is trained on a corpus of text. The corpus is split into pairs of words, where the center word is the word that we want to predict and the context words are the words that surround the center word in the text.\n",
        "\n",
        "The CBOW model uses a neural network to predict the center word for each pair of words. The neural network is trained using the backpropagation algorithm.\n",
        "\n",
        "Once the model has been trained, we can use it to generate word embeddings. To generate a word embedding, we feed the word to the neural network and the neural network will output a vector of numbers. This vector of numbers is the word embedding.\n",
        "\n",
        "Word embeddings can be used to perform a variety of tasks, such as text classification, sentiment analysis, and machine translation. For example, we can use word embeddings to train a classifier that can classify text as positive or negative."
      ],
      "metadata": {
        "id": "giZZUBzDwsKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. Data preparation\n",
        "\n"
      ],
      "metadata": {
        "id": "Cd_-rpq1whPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class CBOWModel:\n",
        "  def __init__(self, vocabulary_size, embedding_size):\n",
        "    self.vocabulary_size = vocabulary_size\n",
        "    self.embedding_size = embedding_size\n",
        "\n",
        "    # Create the embedding layer\n",
        "    self.embeddings = tf.Variable(tf.random.normal([vocabulary_size, embedding_size]))\n",
        "\n",
        "  def predict(self, context_words):\n",
        "    # Convert the context words to embedding vectors\n",
        "    context_word_embeddings = tf.nn.embedding_lookup(self.embeddings, context_words)\n",
        "\n",
        "    # Average the context word embeddings\n",
        "    average_context_word_embedding = tf.reduce_mean(context_word_embeddings, axis=0)\n",
        "\n",
        "    # Predict the center word\n",
        "    center_word_prediction = tf.matmul(average_context_word_embedding, tf.transpose(self.embeddings))\n",
        "\n",
        "    return center_word_prediction\n",
        "\n",
        "\n",
        "# Train the CBOW model\n",
        "model = CBOWModel(vocabulary_size=10000, embedding_size=128)"
      ],
      "metadata": {
        "id": "ovUyHSuOZxIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Generate training data\n",
        "\n"
      ],
      "metadata": {
        "id": "IO6MWZaYwjCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the training data\n",
        "training_data = []\n",
        "for sentence in corpus:\n",
        "  for i in range(len(sentence) - 2):\n",
        "    context_words = [sentence[i - 1], sentence[i + 1]]\n",
        "    center_word = sentence[i]\n",
        "\n",
        "    training_data.append([context_words, center_word])\n"
      ],
      "metadata": {
        "id": "YA0QfBkyZqMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c. Train model\n",
        "\n"
      ],
      "metadata": {
        "id": "Mxg50ONtwkuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(training_data, epochs=10)\n",
        "\n",
        "# Generate word embeddings\n",
        "word_embeddings = model.embeddings.numpy()\n"
      ],
      "metadata": {
        "id": "-cujMXMQZqJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "d. Output"
      ],
      "metadata": {
        "id": "ns3QV8XhwnTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3vAfLbnZwm_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MokrBoLbwm2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Object detection using Transfer Learning of CNN architectures\n"
      ],
      "metadata": {
        "id": "JYfwfRK6ZhkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theory\n",
        "\n",
        "Transfer learning is a machine learning technique where we use a model that has already been trained on one task to train a model for a different task. This can be useful for tasks where we don't have a lot of training data or where it would be difficult and time-consuming to train a model from scratch.\n",
        "\n",
        "CNNs are a type of neural network that is well-suited for image classification and object detection. CNNs are able to learn to extract features from images that are important for classification and detection.\n",
        "\n",
        "When we use transfer learning to train a CNN for object detection, we start by loading a pre-trained CNN model that has been trained on a large dataset, such as ImageNet. We then freeze the parameters in the model's lower convolutional layers. This will prevent us from overfitting the model to our training data.\n",
        "\n",
        "We then add a custom classifier to the model. This classifier will have several layers of trainable parameters. The classifier will be trained to predict the class of each object in the image.\n",
        "\n",
        "We then train the classifier layers on the training data that we have available for the task. This will enable the model to learn to detect the objects in our training data.\n",
        "\n",
        "We may need to fine-tune the hyperparameters of the model, such as the learning rate and the number of epochs, to get the best possible performance. We may also need to unfreeze more layers of the model if it is not performing well."
      ],
      "metadata": {
        "id": "xB4kp_nexWo3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. Load in a pre-trained CNN model trained on a large dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "6z7eRa7GxaWv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2-v9RtnYUb2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "\n",
        "# Load the pre-trained ResNet50 model\n",
        "model = ResNet50(weights='imagenet', include_top=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Freeze parameters(weights) in the model’s lower convolutional layers\n"
      ],
      "metadata": {
        "id": "2eA9_lYMxcbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze the parameters in the model's lower convolutional layers\n",
        "for layer in model.layers[:10]:\n",
        "  layer.trainable = False\n",
        "\n"
      ],
      "metadata": {
        "id": "_j8cSMb1ZyPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "c. Add a custom classifier with several layers of trainable parameters to model\n",
        "\n"
      ],
      "metadata": {
        "id": "GXTgWdNnxe57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a custom classifier to the model\n",
        "classifier = tf.keras.Sequential([\n",
        "  GlobalAveragePooling2D(),\n",
        "  Dense(128, activation='relu'),\n",
        "  Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Attach the classifier to the model\n",
        "model.add(classifier)"
      ],
      "metadata": {
        "id": "7NC2ig6JZyMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "d. Train classifier layers on training data available for the task\n",
        "\n"
      ],
      "metadata": {
        "id": "DB8fYKwGxhEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(x_train, y_train, epochs=10)"
      ],
      "metadata": {
        "id": "dP8BX8ZYxj27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "e. Fine-tune hyperparameters and unfreeze more layers as needed"
      ],
      "metadata": {
        "id": "LCiMswJExkeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "\n",
        "print('Test loss:', test_loss)\n",
        "print('Test accuracy:', test_accuracy)"
      ],
      "metadata": {
        "id": "eVsFLe1qxlC7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}