{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPKMT2BJAT7pGpxGGCHJoaf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sam21sop/BE-AIML-LAB-PRACTICAL/blob/main/Lab_Practice_III_%26_IV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Lab Practice-III (Information Retrieval in AI Lab)\n"
      ],
      "metadata": {
        "id": "ptbx_X9SHYhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Group A:CO1, 2, 3(Any two)\n",
        "1. Implement a Conflation algorithm to generate a document representative of a text file.\n",
        "2. Implement Single-pass Algorithm for the clustering of files. (Consider 4 to 5 files)\n",
        "3. Implement a program for retrieval of documents using inverted files."
      ],
      "metadata": {
        "id": "K_Y6x9wvHpZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a Conflation algorithm to generate a document representative of a text file."
      ],
      "metadata": {
        "id": "hWtuc9LATvsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WblnzyuhTzNo",
        "outputId": "a212805b-ed2e-4f05-f456-cac8cba16e0c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import Counter\n",
        "from nltk.corpus  import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "D4VAoUmBTzQ4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "h9iSBDW-VOcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define stopword and stemmer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "dRLWX6a_TyxV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize the text into words and remove punctuation\n",
        "def preprocess_text(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    words = [word for word in words if word.isalnum()]\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    return words"
      ],
      "metadata": {
        "id": "BhLRWVSBTyud"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess each document and update word frequency\n",
        "def conflate_documents(documents):\n",
        "    word_freq = Counter()\n",
        "    for doc in documents:\n",
        "        with open(doc, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "            words = preprocess_text(content)\n",
        "            word_freq.update(words)\n",
        "    #generate a representative document based on the word frequency\n",
        "    representative_doc = ' '.join([word for word, freq in word_freq.most_common()])\n",
        "    return representative_doc"
      ],
      "metadata": {
        "id": "crx_rUecTyq_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGzJrMQUHXW_"
      },
      "outputs": [],
      "source": [
        "from nltk.data import OpenOnDemandZipFile\n",
        "#list of text input file\n",
        "take_input = ['doc1.txt', 'doc2.txt', 'doc3.txt']\n",
        "\n",
        "#generate the representative document\n",
        "representive_doc = conflate_documents(take_input)\n",
        "\n",
        "#save the representative doc to a file\n",
        "with open('file_name.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(representive_doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Group B: CO3, 5(Any two)\n",
        "1. Implement a program to calculate precision and recall for sample input. (Answer set A, Query q1, Relevant\n",
        "documents to query q1- Rq1 )\n",
        "2. Write a program to calculate the harmonic mean (F-measure) and E-measure for the above example.\n",
        "3. Implement a program for feature extraction in 2D color images (any features like color, texture etc. and\n",
        "extract features from the input image and plot a histogram for the features."
      ],
      "metadata": {
        "id": "DNJHhpYRHyCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qs1nKPM1HmFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Group C:CO4, 5(Any two)\n",
        "1. Build the web crawler to pull product information and links from an e-commerce website. (Python)\n",
        "2. Write a program to find the live weather report (temperature, wind speed, description, and weather) of\n",
        "a given city. (Python).\n",
        "3. Case study on recommender system for a product / Doctor / Product price / Music.\n"
      ],
      "metadata": {
        "id": "2dEwCTsLH4w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gvu0sYFwH9wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab Practice-IV (Deep Learning for AI Lab)"
      ],
      "metadata": {
        "id": "TecfHHMnIF2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mapping of course outcomes for Group A assignments: CO1, CO2, CO3, CO4\n",
        "#### Study of Deep Learning Packages: Tensorflow, Keras, Theano and PyTorch. Document the\n",
        "distinctfeatures and functionality of the packages.\n",
        "\n",
        "Note: Use a suitable dataset forthe implementation of the following assignments."
      ],
      "metadata": {
        "id": "GH9YHpPCIXm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FduLGHPYIMJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implementing Feed-forward neural networks with Keras and TensorFlow\n",
        "a. Import the necessary packages\n",
        "\n",
        "b. Load the training and testing data (MNIST/CIFAR10)\n",
        "\n",
        "c. Define the network architecture using Keras\n",
        "\n",
        "d. Train the model using SGD\n",
        "\n",
        "e. Evaluate the network\n",
        "\n",
        "f. Plot the training loss and accuracy"
      ],
      "metadata": {
        "id": "v3iQBJyZIjRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6adpaSdOIqh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build the Image classification model by dividing the model into the following fourstages:\n",
        "a. Loading and preprocessing the image data\n",
        "\n",
        "b. Defining the model’s architecture\n",
        "\n",
        "c. Training the model\n",
        "\n",
        "d. Estimating the model’s performance"
      ],
      "metadata": {
        "id": "UXb_aL-II5ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-_VlcjO2I5UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Use Autoencoder to implement anomaly detection. Build the model by using the following:\n",
        "a. Import required libraries\n",
        "\n",
        "b. Upload/access the dataset\n",
        "\n",
        "c. The encoder converts it into a latent representation\n",
        "\n",
        "d. Decoder networks convert it back to the original input\n",
        "\n",
        "e. Compile the models with Optimizer, Loss, and Evaluation Metrics"
      ],
      "metadata": {
        "id": "YUvfwELfI5J9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hNHH9OgOI4_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implement the Continuous Bag of Words (CBOW) Model. Stages can be:\n",
        "a. Data preparation\n",
        "\n",
        "b. Generate training data\n",
        "\n",
        "c. Train model\n",
        "\n",
        "d. Output"
      ],
      "metadata": {
        "id": "0RtD3sm5I4zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kD5qpZURI4nV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Object detection using Transfer Learning of CNN architectures\n",
        "a. Load in a pre-trained CNN model trained on a large dataset\n",
        "\n",
        "b. Freeze parameters(weights) in the model’s lower convolutional layers\n",
        "\n",
        "c. Add a custom classifier with several layers of trainable parameters to model\n",
        "\n",
        "d. Train classifier layers on training data available for the task\n",
        "\n",
        "e. Fine-tune hyperparameters and unfreeze more layers as needed"
      ],
      "metadata": {
        "id": "2LFmdXhdI3hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ps4M-0HTJeFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References"
      ],
      "metadata": {
        "id": "ePL4M2hnJ29n"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1kc2eaihJ6MT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}