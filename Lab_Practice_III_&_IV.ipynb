{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNUtbuP/nsa5mococ7Uu+eB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sam21sop/BE-AIML-LAB-PRACTICAL/blob/main/Lab_Practice_III_%26_IV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Lab Practice-III (Information Retrieval in AI Lab)\n"
      ],
      "metadata": {
        "id": "ptbx_X9SHYhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Group A:CO1, 2, 3(Any two)\n",
        "1. Implement a Conflation algorithm to generate a document representative of a text file.\n",
        "2. Implement Single-pass Algorithm for the clustering of files. (Consider 4 to 5 files)\n",
        "3. Implement a program for retrieval of documents using inverted files."
      ],
      "metadata": {
        "id": "K_Y6x9wvHpZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement a Conflation algorithm to generate a document representative of a text file.\n",
        "A conflation algorithm aims to create a representative document by combining information from multiple source documents. Here's a simple Python implementation of a conflation algorithm that takes a list of text files as input and generates a representative document\n",
        "1. This implementation uses the NLTK library for text preprocessing. You can install it using pip install nltk.\n",
        "\n",
        "2. The input text files are assumed to be in the same directory as the script and named document1.txt, document2.txt, etc. You should replace these with actual file paths or modify the script to take the file paths as required.\n",
        "\n",
        "3. The conflation process in this example simply combines the words from the input documents based on their frequency. You can modify the conflation logic to suit your specific requirements, such as considering sentence structures or more advanced language analysis techniques.\n",
        "\n",
        "4. Depending on your use case, you might want to further refine the preprocessing steps and consider more advanced methods for document conflation."
      ],
      "metadata": {
        "id": "hWtuc9LATvsk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mjq6JnYfdUG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "WblnzyuhTzNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import Counter\n",
        "from nltk.corpus  import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "D4VAoUmBTzQ4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "h9iSBDW-VOcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define stopword and stemmer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "dRLWX6a_TyxV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize the text into words and remove punctuation\n",
        "def preprocess_text(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    words = [word for word in words if word.isalnum()]\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    return words"
      ],
      "metadata": {
        "id": "BhLRWVSBTyud"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess each document and update word frequency\n",
        "def conflate_documents(documents):\n",
        "    word_freq = Counter()\n",
        "    for doc in documents:\n",
        "        with open(doc, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "            words = preprocess_text(content)\n",
        "            word_freq.update(words)\n",
        "    #generate a representative document based on the word frequency\n",
        "    representative_doc = ' '.join([word for word, freq in word_freq.most_common()])\n",
        "    return representative_doc"
      ],
      "metadata": {
        "id": "crx_rUecTyq_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGzJrMQUHXW_"
      },
      "outputs": [],
      "source": [
        "from nltk.data import OpenOnDemandZipFile\n",
        "#list of text input file\n",
        "take_input = ['doc1.txt', 'doc2.txt', 'doc3.txt']\n",
        "\n",
        "#generate the representative document\n",
        "representive_doc = conflate_documents(take_input)\n",
        "\n",
        "#save the representative doc to a file\n",
        "with open('file_name.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(representive_doc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YEZSUiNWbBWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BLP1SzQAbivA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement Single-pass Algorithm for the clustering of files. (Consider 4 to 5 files)\n",
        "1. We load and preprocess the text content of each document.\n",
        "2. The TF-IDF vectorizer is used to convert the documents into numerical vectors.\n",
        "2. We initialize K-Means clustering with the desired number of clusters.\n",
        "3. We iterate through each document and compute its cosine similarity with existing cluster centroids.\n",
        "4. The document is assigned to the cluster with the highest similarity, and the cluster centroid is updated.\n",
        "\n"
      ],
      "metadata": {
        "id": "u3j7Es86bBzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "l9K59O3HbIDR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess documents\n",
        "documents = []\n",
        "file_paths = ['document1.txt', 'document2.txt', 'document3.txt', 'document4.txt', 'document5.txt']\n",
        "\n",
        "for file_path in file_paths:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "        documents.append(content)\n"
      ],
      "metadata": {
        "id": "iYr8Ff31bIAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert documents to TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)"
      ],
      "metadata": {
        "id": "pWhXMs0abH95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize K-Means clustering with the desired number of clusters\n",
        "num_clusters = 2  # You can adjust this number\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n"
      ],
      "metadata": {
        "id": "NduDqhbYbH7E"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process documents and perform single-pass clustering\n",
        "cluster_assignments = []\n",
        "for i in range(len(documents)):\n",
        "    document_vector = tfidf_matrix[i]\n",
        "\n",
        "    # Compute cosine similarity with existing cluster centroids\n",
        "    similarities = cosine_similarity(document_vector, kmeans.cluster_centers_)\n",
        "\n",
        "    # Assign document to the most similar cluster\n",
        "    closest_cluster = np.argmax(similarities)\n",
        "    cluster_assignments.append(closest_cluster)\n",
        "\n",
        "    # Update the cluster centroid\n",
        "    kmeans.cluster_centers_[closest_cluster] = (kmeans.cluster_centers_[closest_cluster] * (i + 1) + document_vector) / (i + 2)\n"
      ],
      "metadata": {
        "id": "5_Ijo_T6bH4N"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print cluster assignments\n",
        "for i, cluster_id in enumerate(cluster_assignments):\n",
        "    print(f\"Document {file_paths[i]} belongs to Cluster {cluster_id}\")"
      ],
      "metadata": {
        "id": "xNh5NN2vbH1n"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F07ZheobbHyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S4YozlRIbHv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement a program for retrieval of documents using inverted files.\n",
        "\n",
        "1. We preprocess the text of each document to tokenize it into words and remove punctuation.\n",
        "\n",
        "2. The build_inverted_index function creates an inverted index using a defaultdict. For each word, it maps the word to the document IDs where the word appears.\n",
        "\n",
        "3. The retrieve_documents function takes a query and retrieves documents containing any of the query terms based on the inverted index.\n",
        "\n",
        "4. We provide a sample set of documents and a sample user query to demonstrate the retrieval process."
      ],
      "metadata": {
        "id": "cXc6bSY1bkiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "lbm2oBYbbtbk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    '''Tokenize the text into words and remove punctuation'''\n",
        "    words = re.findall(r'\\w+', text.lower())\n",
        "    return words"
      ],
      "metadata": {
        "id": "MLTvtPnZbtYj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_inverted_index(documents):\n",
        "    inverted_index = defaultdict(list)\n",
        "    for doc_id, document in enumerate(documents):\n",
        "        words = preprocess_text(document)\n",
        "        for word in words:\n",
        "            inverted_index[word].append(doc_id)\n",
        "    return inverted_index"
      ],
      "metadata": {
        "id": "H9tj5ZtybtVf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_documents(inverted_index, query):\n",
        "    query_terms = preprocess_text(query)\n",
        "    matching_doc_ids = set()\n",
        "    for term in query_terms:\n",
        "        if term in inverted_index:\n",
        "            matching_doc_ids.update(inverted_index[term])\n",
        "    return matching_doc_ids"
      ],
      "metadata": {
        "id": "8q3isUyhbtEe"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample documents\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"The second document is here.\",\n",
        "    \"And here is the third document.\",\n",
        "    \"This document contains important information.\",\n",
        "    \"The fifth document concludes the set.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "CsvInwWebtB0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build inverted index\n",
        "inverted_index = build_inverted_index(documents)"
      ],
      "metadata": {
        "id": "yiS5qMLfbs-X"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# User query\n",
        "query = \"document information\""
      ],
      "metadata": {
        "id": "HifiJ34dbs7A"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve matching documents\n",
        "matching_doc_ids = retrieve_documents(inverted_index, query)"
      ],
      "metadata": {
        "id": "MAOBzRjIbs3-"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print matching documents\n",
        "if matching_doc_ids:\n",
        "    print(\"Matching Documents:\")\n",
        "    for doc_id in matching_doc_ids:\n",
        "        print(f\"Document {doc_id + 1}: {documents[doc_id]}\")\n",
        "else:\n",
        "    print(\"No matching documents found.\")"
      ],
      "metadata": {
        "id": "q5sj4NpvcaIn",
        "outputId": "94b8ff37-5974-48ed-e0f5-f9934b2e8eea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matching Documents:\n",
            "Document 1: This is the first document.\n",
            "Document 2: The second document is here.\n",
            "Document 3: And here is the third document.\n",
            "Document 4: This document contains important information.\n",
            "Document 5: The fifth document concludes the set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x9tiMqS6cZ00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dD3WKzv2cZxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Group B: CO3, 5(Any two)\n",
        "1. Implement a program to calculate precision and recall for sample input. (Answer set A, Query q1, Relevant\n",
        "documents to query q1- Rq1 )\n",
        "2. Write a program to calculate the harmonic mean (F-measure) and E-measure for the above example.\n",
        "3. Implement a program for feature extraction in 2D color images (any features like color, texture etc. and\n",
        "extract features from the input image and plot a histogram for the features."
      ],
      "metadata": {
        "id": "DNJHhpYRHyCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qs1nKPM1HmFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Group C:CO4, 5(Any two)\n",
        "1. Build the web crawler to pull product information and links from an e-commerce website. (Python)\n",
        "2. Write a program to find the live weather report (temperature, wind speed, description, and weather) of\n",
        "a given city. (Python).\n",
        "3. Case study on recommender system for a product / Doctor / Product price / Music.\n"
      ],
      "metadata": {
        "id": "2dEwCTsLH4w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gvu0sYFwH9wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab Practice-IV (Deep Learning for AI Lab)"
      ],
      "metadata": {
        "id": "TecfHHMnIF2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mapping of course outcomes for Group A assignments: CO1, CO2, CO3, CO4\n",
        "#### Study of Deep Learning Packages: Tensorflow, Keras, Theano and PyTorch. Document the\n",
        "distinctfeatures and functionality of the packages.\n",
        "\n",
        "Note: Use a suitable dataset forthe implementation of the following assignments."
      ],
      "metadata": {
        "id": "GH9YHpPCIXm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FduLGHPYIMJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implementing Feed-forward neural networks with Keras and TensorFlow\n",
        "a. Import the necessary packages\n",
        "\n",
        "b. Load the training and testing data (MNIST/CIFAR10)\n",
        "\n",
        "c. Define the network architecture using Keras\n",
        "\n",
        "d. Train the model using SGD\n",
        "\n",
        "e. Evaluate the network\n",
        "\n",
        "f. Plot the training loss and accuracy"
      ],
      "metadata": {
        "id": "v3iQBJyZIjRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6adpaSdOIqh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build the Image classification model by dividing the model into the following fourstages:\n",
        "a. Loading and preprocessing the image data\n",
        "\n",
        "b. Defining the model’s architecture\n",
        "\n",
        "c. Training the model\n",
        "\n",
        "d. Estimating the model’s performance"
      ],
      "metadata": {
        "id": "UXb_aL-II5ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-_VlcjO2I5UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Use Autoencoder to implement anomaly detection. Build the model by using the following:\n",
        "a. Import required libraries\n",
        "\n",
        "b. Upload/access the dataset\n",
        "\n",
        "c. The encoder converts it into a latent representation\n",
        "\n",
        "d. Decoder networks convert it back to the original input\n",
        "\n",
        "e. Compile the models with Optimizer, Loss, and Evaluation Metrics"
      ],
      "metadata": {
        "id": "YUvfwELfI5J9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hNHH9OgOI4_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implement the Continuous Bag of Words (CBOW) Model. Stages can be:\n",
        "a. Data preparation\n",
        "\n",
        "b. Generate training data\n",
        "\n",
        "c. Train model\n",
        "\n",
        "d. Output"
      ],
      "metadata": {
        "id": "0RtD3sm5I4zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kD5qpZURI4nV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Object detection using Transfer Learning of CNN architectures\n",
        "a. Load in a pre-trained CNN model trained on a large dataset\n",
        "\n",
        "b. Freeze parameters(weights) in the model’s lower convolutional layers\n",
        "\n",
        "c. Add a custom classifier with several layers of trainable parameters to model\n",
        "\n",
        "d. Train classifier layers on training data available for the task\n",
        "\n",
        "e. Fine-tune hyperparameters and unfreeze more layers as needed"
      ],
      "metadata": {
        "id": "2LFmdXhdI3hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ps4M-0HTJeFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References"
      ],
      "metadata": {
        "id": "ePL4M2hnJ29n"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1kc2eaihJ6MT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}