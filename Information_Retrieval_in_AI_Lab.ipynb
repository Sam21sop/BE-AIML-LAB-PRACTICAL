{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7gJbMytxTBkv",
        "2UQ7EdEQTOoD",
        "TrCc_CqHTIuV",
        "pzVH4sdGVSxg",
        "A0PBQ1w4VIya"
      ],
      "authorship_tag": "ABX9TyO9D6kGpxDNIPc3BJ7faBab"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Lab Practice-III (Information Retrieval in AI Lab)\n"
      ],
      "metadata": {
        "id": "ptbx_X9SHYhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Group A:CO1, 2, 3(Any two)\n"
      ],
      "metadata": {
        "id": "K_Y6x9wvHpZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Implement a Conflation algorithm to generate a document representative of a text file.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7gJbMytxTBkv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conflation Algorithm**\n",
        "\n",
        "The conflation algorithm is a statistical method for generating a document representative of a text file. It works by identifying and grouping together words that are semantically related. This is done by removing affixes from words and identifying words that have similar meanings.\n",
        "\n",
        "The conflation algorithm can be implemented in the following steps:\n",
        "\n",
        "1. **Tokenize the text.** This involves splitting the text into individual words and punctuation marks.\n",
        "\n",
        "2. **Remove stop words**. Stop words are common words that do not add much meaning to a document, such as \"the\", \"is\", and \"of\".\n",
        "\n",
        "3. **Stem the words.** This involves removing affixes from words, such as prefixes and suffixes. This can be done using a variety of stemming algorithms, such as the Porter stemmer or the Snowball stemmer.\n",
        "\n",
        "4. **Identify synonyms.** This can be done using a variety of methods, such as using a wordnet or using a statistical method to identify words that have similar meanings.\n",
        "\n",
        "5. **Group the words into classes.** This can be done using a variety of methods, such as clustering or using a statistical method to identify groups of words that are semantically related.\n",
        "\n",
        "6. **Generate the document representative.** This can be done by selecting the most representative words from each class, or by generating a weighted vector of words, where the weight of a word represents its importance to the document."
      ],
      "metadata": {
        "id": "yRQbbWz5TBh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theory**\n",
        "\n",
        "The conflation algorithm is based on the assumption that semantically related words will tend to occur together in documents. This is because words that have similar meanings are often used to describe the same concepts.\n",
        "\n",
        "The conflation algorithm works by grouping together words that are semantically related. This is done by removing affixes from words and identifying words that have similar meanings.\n",
        "\n",
        "Removing affixes from words helps to identify the root of a word, which is often the most informative part of a word. For example, the words \"running\", \"ran\", and \"runs\" all have the same root word, \"run\". By removing the affixes from these words, we can group them together as semantically related words.\n",
        "\n",
        "Identifying words that have similar meanings is more difficult, but there are a variety of methods that can be used. One common method is to use a wordnet. A wordnet is a database of words that are linked together by their semantic relationships. For example, the word \"dog\" is linked to the words \"animal\", \"canine\", and \"puppy\" in a wordnet.\n",
        "\n",
        "Another method for identifying words that have similar meanings is to use a statistical method. One common statistical method is to use the cosine similarity between words. The cosine similarity between two words is a measure of how similar the two words are in terms of their meaning.\n",
        "\n",
        "Once the words have been grouped together into classes, the document representative can be generated. This can be done by selecting the most representative words from each class, or by generating a weighted vector of words, where the weight of a word represents its importance to the document."
      ],
      "metadata": {
        "id": "qTHRp8ruT1NO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "class ConflationAlgorithm:\n",
        "    def __init__(self, stop_words=None):\n",
        "        self.stop_words = stop_words or set()\n",
        "        self.stemmer = nltk.stem.PorterStemmer()\n",
        "\n",
        "    def conflate(self, text):\n",
        "        # Tokenize the text.\n",
        "        tokens = re.split(r'\\s+', text)\n",
        "\n",
        "        # Remove stop words.\n",
        "        tokens = [token for token in tokens if token not in self.stop_words]\n",
        "\n",
        "        # Stem the words.\n",
        "        tokens = [self.stemmer.stem(token) for token in tokens]\n",
        "\n",
        "        # Identify synonyms.\n",
        "        synonyms = {}\n",
        "        for token in tokens:\n",
        "            synonyms.setdefault(token, set()).add(token)\n",
        "\n",
        "            # TODO: Use a wordnet or statistical method to identify synonyms.\n",
        "\n",
        "        # Group the words into classes.\n",
        "        classes = []\n",
        "        for token in tokens:\n",
        "            class_found = False\n",
        "            for class_ in classes:\n",
        "                if token in class_:\n",
        "                    class_.add(token)\n",
        "                    class_found = True\n",
        "                    break\n",
        "\n",
        "            if not class_found:\n",
        "                classes.append(set([token]))\n",
        "\n",
        "        # Generate the document representative.\n",
        "        document_representative = []\n",
        "        for class_ in classes:\n",
        "            document_representative.append(max(class_, key=len))\n",
        "\n",
        "        return document_representative\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "conflation_algorithm = ConflationAlgorithm()\n",
        "document_representative = conflation_algorithm.conflate(\"This is a test document.\")\n",
        "\n",
        "print(document_representative)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWizMxRyTBK6",
        "outputId": "2912eeac-7fe2-40fd-8757-dd166879cbfa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thi', 'is', 'a', 'test', 'document.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IJ6n7i4iTBH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Implement Single-pass Algorithm for the clustering of files. (Consider 4 to 5 files).\n"
      ],
      "metadata": {
        "id": "2UQ7EdEQTOoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "G1todmoUUSE3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PWHQjs2uTA4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yXpavs8KTA1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Implement a program for retrieval of documents using inverted files."
      ],
      "metadata": {
        "id": "TrCc_CqHTIuV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theory**\n",
        "\n",
        "An inverted index is a data structure that maps words to the documents in which they appear. It is a common data structure used in information retrieval systems, such as search engines.\n",
        "\n",
        "To create an inverted index, we first need to tokenize the documents, which means splitting them into individual words. We then need to remove stop words, which are common words that do not add much meaning to a document, such as \"the\", \"is\", and \"of\".\n",
        "\n",
        "Once we have tokenized and removed stop words, we can start building the inverted index. For each word in each document, we add the document ID to the inverted index entry for that word.\n",
        "\n",
        "To search for documents using an inverted index, we simply need to split the query into words and then look up each word in the inverted index. The documents that contain all of the query words will be the results of the search.\n",
        "\n",
        "Inverted indexes are very efficient for searching large collections of documents. This is because we can quickly find all of the documents that contain a given word by simply looking up the word in the inverted index.\n",
        "\n",
        "Advantages of using inverted indexes:\n",
        "\n",
        "1. Inverted indexes are very efficient for searching large collections of documents.\n",
        "2. Inverted indexes can be used to implement a variety of search features, such as Boolean search, phrase search, and proximity search.\n",
        "3. Inverted indexes are relatively easy to implement.\n",
        "\n",
        "Disadvantages of using inverted indexes:\n",
        "\n",
        "1. Inverted indexes can be large, especially for large collections of documents.\n",
        "Inverted indexes need to be updated whenever a document is added to or removed from the collection.\n",
        "2. Inverted indexes can be complex to implement for more advanced search features, such as fuzzy search."
      ],
      "metadata": {
        "id": "G64Y5HbNUa3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class InvertedIndex:\n",
        "    def __init__(self):\n",
        "        self.index = {}\n",
        "\n",
        "    def add_document(self, document_id, document):\n",
        "        for word in document.split():\n",
        "            if word not in self.index:\n",
        "                self.index[word] = set()\n",
        "            self.index[word].add(document_id)\n",
        "\n",
        "    def search(self, query):\n",
        "        results = set()\n",
        "        for word in query.split():\n",
        "            if word in self.index:\n",
        "                results.update(self.index[word])\n",
        "        return results\n",
        "\n",
        "\n",
        "def main():\n",
        "    inverted_index = InvertedIndex()\n",
        "\n",
        "    # Add documents to the inverted index.\n",
        "    inverted_index.add_document(1, \"This is the first document.\")\n",
        "    inverted_index.add_document(2, \"This is the second document.\")\n",
        "    inverted_index.add_document(3, \"This is the third document.\")\n",
        "\n",
        "    # Search for documents containing the query \"document\".\n",
        "    results = inverted_index.search(\"document\")\n",
        "\n",
        "    # Print the results.\n",
        "    for document_id in results:\n",
        "        print(document_id)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "f43boBefTAyQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w_PXDTIVTAvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Group B: CO3, 5(Any two)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DNJHhpYRHyCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Implement a program to calculate precision and recall for sample input. (Answer set A, Query q1, Relevant documents to query q1- Rq1 )"
      ],
      "metadata": {
        "id": "pzVH4sdGVSxg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theory**\n",
        "\n",
        "Precision and recall are two important metrics used to evaluate the performance of information retrieval systems.\n",
        "\n",
        "Precision is the fraction of retrieved documents that are relevant to the user's query. Recall is the fraction of relevant documents that are retrieved.\n",
        "\n",
        "Precision and recall are often at odds with each other. If we want to improve precision, we can simply retrieve fewer documents. However, this will reduce recall, because we will be missing out on some relevant documents.\n",
        "\n",
        "To achieve a good balance between precision and recall, we need to use a variety of techniques, such as ranking the retrieved documents by relevance and using a relevance threshold.\n",
        "\n",
        "Advantages of using precision and recall:\n",
        "\n",
        "1. Precision and recall are easy to understand and calculate.\n",
        "2. Precision and recall can be used to compare the performance of different information retrieval systems.\n",
        "3. Precision and recall can be used to identify the areas where an information retrieval system needs to be improved.\n",
        "\n",
        "Disadvantages of using precision and recall:\n",
        "\n",
        "1. Precision and recall are not the only metrics that are important for evaluating the performance of information retrieval systems. Other metrics, such as user satisfaction, are also important.\n",
        "2. Precision and recall can be biased towards certain types of queries. For example, precision and recall are often higher for short and specific queries than for long and general queries."
      ],
      "metadata": {
        "id": "h9J4KQvxVY_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_precision_and_recall(relevant_documents, retrieved_documents):\n",
        "  \"\"\"Calculates the precision and recall for a given set of relevant and retrieved documents.\n",
        "\n",
        "  Args:\n",
        "    relevant_documents: A set of relevant documents.\n",
        "    retrieved_documents: A set of retrieved documents.\n",
        "\n",
        "  Returns:\n",
        "    A tuple of (precision, recall).\n",
        "  \"\"\"\n",
        "\n",
        "  precision = len(relevant_documents & retrieved_documents) / len(retrieved_documents)\n",
        "  recall = len(relevant_documents & retrieved_documents) / len(relevant_documents)\n",
        "  return precision, recall\n",
        "\n",
        "\n",
        "# Sample input:\n",
        "\n",
        "relevant_documents = {\"1\", \"2\", \"3\"}\n",
        "retrieved_documents = {\"1\", \"2\", \"4\"}\n",
        "\n",
        "# Calculate precision and recall.\n",
        "\n",
        "precision, recall = calculate_precision_and_recall(relevant_documents, retrieved_documents)\n",
        "\n",
        "# Print the results.\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0wJ374tVIfC",
        "outputId": "aacf7abc-a5e7-4993-b5d0-f388cf89a8a6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.6666666666666666\n",
            "Recall: 0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1gofnwqLVIQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Write a program to calculate the harmonic mean (F-measure) and E-measure for the above example."
      ],
      "metadata": {
        "id": "A0PBQ1w4VIya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theory**\n",
        "\n",
        "The harmonic mean is a way of combining two numbers into a single number, where the two numbers are weighted equally. The harmonic mean of two numbers x and y is calculated as follows:\n",
        "\n",
        " **harmonic_mean(x, y) = 2 * xy / (x + y)**\n",
        "\n",
        "The F-measure is a harmonic mean of precision and recall, weighted by the beta parameter. The beta parameter controls the importance of precision relative to recall. A higher beta parameter will give more weight to precision, while a lower beta parameter will give more weight to recall.\n",
        "\n",
        "The E-measure is another harmonic mean of precision and recall, which is equivalent to the F-measure when the beta parameter is equal to 1.\n",
        "\n",
        "Advantages of using the F-measure and E-measure:\n",
        "\n",
        "1. The F-measure and E-measure are single metrics that combine precision and recall into a single number. This makes them easy to understand and interpret.\n",
        "2. The F-measure and E-measure can be used to compare the performance of different information retrieval systems.\n",
        "3. The F-measure and E-measure can be used to identify the areas where an information retrieval system needs to be improved.\n",
        "\n",
        "Disadvantages of using the F-measure and E-measure:\n",
        "\n",
        "1. The F-measure and E-measure are biased towards certain types of queries. For example, the F-measure and E-measure are often higher for short and specific queries than for long and general queries.\n",
        "2. The F-measure and E-measure do not take into account other important factors, such as user satisfaction."
      ],
      "metadata": {
        "id": "xb3Pt6ZdWEPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_f_measure(precision, recall, beta=1):\n",
        "  \"\"\"Calculates the F-measure for a given precision and recall.\n",
        "\n",
        "  Args:\n",
        "    precision: The precision.\n",
        "    recall: The recall.\n",
        "    beta: The beta parameter.\n",
        "\n",
        "  Returns:\n",
        "    The F-measure.\n",
        "  \"\"\"\n",
        "\n",
        "  f_measure = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n",
        "  return f_measure\n",
        "\n",
        "\n",
        "def calculate_e_measure(precision, recall, beta=1):\n",
        "  \"\"\"Calculates the E-measure for a given precision and recall.\n",
        "\n",
        "  Args:\n",
        "    precision: The precision.\n",
        "    recall: The recall.\n",
        "    beta: The beta parameter.\n",
        "\n",
        "  Returns:\n",
        "    The E-measure.\n",
        "  \"\"\"\n",
        "\n",
        "  e_measure = (1 + beta**2) * (precision * recall) / (precision + (beta**2 * recall))\n",
        "  return e_measure\n",
        "\n",
        "\n",
        "# Sample input:\n",
        "\n",
        "precision = 0.6666666666666666\n",
        "recall = 0.6666666666666666\n",
        "\n",
        "# Calculate F-measure and E-measure.\n",
        "\n",
        "f_measure = calculate_f_measure(precision, recall)\n",
        "e_measure = calculate_e_measure(precision, recall)\n",
        "\n",
        "# Print the results.\n",
        "\n",
        "print(\"F-measure:\", f_measure)\n",
        "print(\"E-measure:\", e_measure)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp2OkDnQVINv",
        "outputId": "abb0147a-edaf-4663-b5d2-5b5d2574c184"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-measure: 0.6666666666666666\n",
            "E-measure: 0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C0gsBBQMVIKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Group C:CO4(Any two)\n",
        "\n"
      ],
      "metadata": {
        "id": "2dEwCTsLH4w_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Build the web crawler to pull product information and links from an e-commerce website. (Python)\n"
      ],
      "metadata": {
        "id": "_9ZMxLaIa-5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To build a web crawler to pull product information and links from an e-commerce website, you can follow these steps:\n",
        "\n",
        "1. **Choose a programming language.** Python is a popular choice for web crawling, due to its simplicity and the availability of many libraries for web scraping.\n",
        "2. **Install the necessary libraries.** For Python, you will need to install the following libraries:\n",
        "\n",
        "> *   requests: This library allows you to make HTTP requests to websites.\n",
        "*   BeautifulSoup: This library allows you to parse HTML and extract data from it.\n",
        "\n",
        "3. **Identify the start URLs.** These are the URLs that your crawler will start crawling from. For example, if you want to crawl the Amazon website, you could start with the following start URL: https://www.amazon.com/.\n",
        "4. **Write a function to crawl a URL.** This function should take a URL as input and return a list of URLs that it found on the page. You can use the requests library to download the HTML of the page and the BeautifulSoup library to parse it and extract the links.\n",
        "5. **Write a function to extract product information.** This function should take a URL as input and return a dictionary of product information, such as the product name, price, and description. You can use the BeautifulSoup library to parse the HTML of the page and extract the product information.\n",
        "6. **Write a main function.** This function should initialize the crawler and then start crawling the start URLs. It should also call the function to extract product information for each product that it finds."
      ],
      "metadata": {
        "id": "6FCqJ7Z-bS76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install BeautifulSoup"
      ],
      "metadata": {
        "id": "FZ3_AlO8cg8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "class WebCrawler:\n",
        "    def __init__(self, start_urls):\n",
        "        self.start_urls = start_urls\n",
        "        self.crawled_urls = set()\n",
        "\n",
        "    def crawl(self):\n",
        "        for url in self.start_urls:\n",
        "            if url not in self.crawled_urls:\n",
        "                self.crawled_urls.add(url)\n",
        "                response = requests.get(url)\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "                # Extract product information\n",
        "                product_information = {}\n",
        "                product_information['name'] = soup.find('h1', class_='product-title').text\n",
        "                product_information['price'] = soup.find('span', class_='price-block__final-price').text\n",
        "                product_information['description'] = soup.find('div', id='product-description').text\n",
        "\n",
        "                # Save the product information\n",
        "                # ...\n",
        "\n",
        "                # Extract links on the page\n",
        "                links = []\n",
        "                for link in soup.find_all('a'):\n",
        "                    links.append(link['href'])\n",
        "\n",
        "                # Add the links to the crawler's queue\n",
        "                for link in links:\n",
        "                    self.crawl(link)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    crawler = WebCrawler(['https://www.amazon.com/'])\n",
        "    crawler.crawl()"
      ],
      "metadata": {
        "id": "kh3fWLjra5Sl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Write a program to find the live weather report (temperature, wind speed, description, and weather) of a given city. (Python).\n"
      ],
      "metadata": {
        "id": "UWRptzDrbC5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To write a program to find the live weather report (temperature, wind speed, description, and weather) of a given city in Python, you can follow these steps:\n",
        "\n",
        "1. **Install the necessary libraries**. You will need to install the following Python libraries:\n",
        "\n",
        "\n",
        "> * requests: This library allows you to make HTTP requests to websites.\n",
        "* json: This library allows you to parse JSON data.\n",
        "\n",
        "2. **Identify the weather API that you want to use**. There are many different weather APIs available, such as OpenWeatherMap and Dark Sky. Choose an API that provides the data that you need in a format that you can easily parse.\n",
        "3. **Write a function to make a request to the weather API**. This function should take the city name as input and return a JSON object containing the weather data. You can use the requests library to make the HTTP request and the json library to parse the JSON response.\n",
        "4. **Write a function to extract the weather information from the JSON object**. This function should take the JSON object as input and return a dictionary containing the temperature, wind speed, description, and weather.\n",
        "5. **Write a main function**. This function should initialize the program and then call the function to get the weather information for the given city. It should then print the weather information to the console."
      ],
      "metadata": {
        "id": "_zw88Kv6c51x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def get_weather_report(city_name):\n",
        "    api_key = 'YOUR_API_KEY'\n",
        "    url = f'https://api.openweathermap.org/data/2.5/weather?q={city_name}&appid={api_key}'\n",
        "\n",
        "    response = requests.get(url)\n",
        "    weather_data = json.loads(response.content)\n",
        "\n",
        "    return weather_data['main']['temp'], weather_data['wind']['speed'], weather_data['weather'][0]['description'], weather_data['weather'][0]['main']\n",
        "\n",
        "def main():\n",
        "    city_name = input('Enter the city name: ')\n",
        "\n",
        "    weather_report = get_weather_report(city_name)\n",
        "\n",
        "    temperature = weather_report[0]\n",
        "    wind_speed = weather_report[1]\n",
        "    description = weather_report[2]\n",
        "    weather = weather_report[3]\n",
        "\n",
        "    print(f'The weather in {city_name} is {weather} with a temperature of {temperature} degrees Celsius and a wind speed of {wind_speed} meters per second.')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "owTiTAffa5E4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Case study on recommender system for a product / Doctor / Product price / Music."
      ],
      "metadata": {
        "id": "-QYSb-HdbIJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Case study on recommender system for music with theory in details\n",
        "\n",
        "**Introduction**\n",
        "\n",
        "Recommender systems are used to suggest items to users based on their past behavior or preferences. They are used in a variety of applications, such as e-commerce, streaming services, and social media.\n",
        "\n",
        "Music recommender systems suggest songs or playlists to users based on their listening habits, preferences, and other factors. They can be used to help users discover new music, find similar songs to the ones they already like, and create personalized playlists.\n",
        "\n",
        "**Theory**\n",
        "\n",
        "There are two main types of music recommender systems: collaborative filtering and content-based filtering.\n",
        "\n",
        "Collaborative filtering recommender systems suggest items to users based on the preferences of other users who have similar tastes. For example, if a user likes to listen to pop music, and other users who like pop music also like to listen to rock music, then a collaborative filtering recommender system might suggest rock music to that user.\n",
        "\n",
        "Content-based filtering recommender systems suggest items to users based on the features of the items themselves. For example, if a user likes to listen to songs with a fast tempo and loud vocals, then a content-based filtering recommender system might suggest other songs with similar features.\n",
        "\n",
        "**Case study**\n",
        "\n",
        "Spotify is a music streaming service that uses a combination of collaborative filtering and content-based filtering to recommend songs to its users.\n",
        "\n",
        "Spotify's collaborative filtering recommender system is based on the idea that users who have similar listening habits are more likely to like the same songs. Spotify uses this information to generate personalized recommendations for each user.\n",
        "\n",
        "Spotify's content-based filtering recommender system is based on the idea that songs with similar features are more likely to be liked by the same users. Spotify uses a variety of features, such as genre, tempo, and mood, to generate personalized recommendations for each user.\n",
        "\n",
        "Spotify's recommender system is very effective at helping users discover new music and find songs that they like. It is also very efficient, as it can generate personalized recommendations for millions of users in real time.\n",
        "\n",
        "**Benefits of music recommender systems**\n",
        "\n",
        "Music recommender systems offer a number of benefits, including:\n",
        "\n",
        "Discovery: Music recommender systems can help users discover new music that they might not have otherwise found.\n",
        "Personalization: Music recommender systems can create personalized recommendations for each user, based on their individual tastes and preferences.\n",
        "Convenience: Music recommender systems can save users time and effort by suggesting songs that they are likely to enjoy.\n",
        "\n",
        "**Challenges of music recommender systems**\n",
        "\n",
        "One of the biggest challenges of music recommender systems is that they can be biased. This is because they are trained on data that is collected from real users, and this data can reflect the biases of those users.\n",
        "\n",
        "Another challenge of music recommender systems is that they can be difficult to evaluate. This is because there is no objective way to measure how well a music recommender system is working.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "Music recommender systems are a powerful tool for helping users discover new music and find songs that they like. However, it is important to be aware of the biases that can exist in music recommender systems and the challenges of evaluating their performance."
      ],
      "metadata": {
        "id": "6E2qJrSUdubn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gvu0sYFwH9wH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References"
      ],
      "metadata": {
        "id": "ePL4M2hnJ29n"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1kc2eaihJ6MT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}